model:
  wide: True

  deep: True
  mlp_layer: [64, 64, 64]
  activation: relu

  dropout: 0.3
  attention_dim: 16
  num_attention_layers: 5
  n_head: 16
  
  residual: True
  residual_project: True
  layer_norm: True

train:
  learning_rate: 0.001
