model:
  embed_dim: 8
  mlp_layer: [64,64,64]
  activation: relu
  dropout: 0.2

train: 
  learning_rate: 0.0005
