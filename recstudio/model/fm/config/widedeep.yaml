model:
  batch_norm: False
  embed_dim: 8
  mlp_layer: [128,128,128]
  activation: dice
  dropout: 0.1

train:
  learning_rate: 0.001
