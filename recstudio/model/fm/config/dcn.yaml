model:
  embed_dim: 16
  mlp_layer: [128,128,128]
  activation: relu
  num_layers: 6
  dropout: 0.2
  batch_norm: true

train:
  learning_rate: 0.01